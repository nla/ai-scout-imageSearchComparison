#!/usr/bin/env python3

# kf13sep24
# uses files generated by https://hinton.nla.gov.au:4321/admin/generateFilesForClustering
# hacked from Francis' https://github.com/nla/gen_ai/blob/main/src/cluster.py
# run like this:  python3 cluster.py  data > clusterRunLog  (defaults - only 3 clusters!? - 5358 items out of 6009 items) (24 sec)
# python3 cluster.py  data -m 10 -e 50 > clusterRunLog2   (found 37 clusters, 1498 items)
# python3 cluster.py  data -m 5 -e 50 > clusterRunLog3   (found only 3 clusters!)  (23 sec)
# python3 cluster.py  data -m 8 -e 100 > clusterRunLog4  (found 47 clusters - 1554 items)  going to use this one!
# python3 cluster.py  data -m 20 -e 100 > clusterRunLog5 (found 20 clusters - 1545 items)
# python3 cluster.py  data -m 15 -e 100 > clusterRunLog6 (found 24 clusters - 1426 items)

import argparse
import json
import os
from datetime import datetime

import hdbscan
import numpy as np
from sklearn.cluster import KMeans

# from domain.metadata import (METADATA_FILE, TITLE, URL, FILE_PATH, CLUSTER_ID, CLUSTER_SIZE, CLUSTER_URL,
#                             SUMMARY_EMBEDDING)
# from src.index_records import clean_metadata

HDBSCAN = "hdbscan"
KMEANS = "kmeans"


def main():
    parser = argparse.ArgumentParser(description="Cluster embedding (vector) data")
    parser.add_argument("data_root", type=str, help="Root of file tree to load metadata JSON files from")
    parser.add_argument('-a', "--algorithm", type=str, default=HDBSCAN,
                        help="Clustering algorithm (hdbscan, kmeans) - default is hdbscan")
    parser.add_argument('-m', "--min_cluster_size", type=int, default="5",
                        help="Minimum cluster size (required for hdbscan) - default is 5")
    parser.add_argument('-e', "--expected_num_clusters", type=int, default="10",
                        help="Expected number of clusters (required for kmeans) - default is 10")
    parser.add_argument('-u', "--update_records", action='store_true',
                        help="Update metadata records with cluster ID and cluster size for each record in a cluster")
    

    args = parser.parse_args()
    process_file_tree(args.data_root, args.min_cluster_size, args.algorithm, args.expected_num_clusters,
                      args.update_records)


def process_file_tree(data_root, min_cluster_size, algorithm, expected_num_clusters, update_records):
    formatted_datetime = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"Starting clustering from: {data_root} at {formatted_datetime}")

    embeddings_array, metadata_list = get_embeddings_and_metadata(data_root)
    clusters = get_clusters(embeddings_array, algorithm, min_cluster_size, expected_num_clusters)

    if clusters:
        print(f"Number of clusters found: {clusters.labels_.max() + 1}\n")
        sorted_clusters = sort_clusters(clusters, embeddings_array, metadata_list, update_records)
        total_clustered_items = 0

        for cluster_size, cluster_id, metadata in sorted_clusters:
            print(f"Cluster {cluster_id} - Size: {cluster_size},  URL: {metadata['id']}")
            total_clustered_items += cluster_size

        print(f"\nTotal clustered items: {total_clustered_items}")
    else:
        print("No clusters generated.")

    formatted_datetime = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"Finished clustering at: {formatted_datetime}")


def get_embeddings_and_metadata(data_root):
    embeddings = []
    records_metadata = []

    for root, dirs, files in os.walk(data_root):
        for file in files:
            if file.endswith(".json"):
                file_path = os.path.join(root, file)

                try:
                    metadata = get_metadata(file_path)

                    if metadata and metadata["clipEmbedding"]:
                        # print( "read metadata " + file_path + " id " + metadata["id"])
                        embeddings.append(metadata["clipEmbedding"])
                        metadata["file_path"] = file_path
                        records_metadata.append(metadata)
                except Exception as e:
                    print(f"Error reading {file_path}: {e}")

    return np.array(embeddings), records_metadata


def get_clusters(embeddings_array, algorithm, min_cluster_size, expected_num_clusters, min_samples=3):
    print(f"Total items to cluster: {embeddings_array.shape[0]}")
    print(f"Clustering algorithm: {algorithm}")

    if algorithm == HDBSCAN:
        print(f"Minimum cluster size: {min_cluster_size}")
        print(f"Minimum samples: {min_samples}")
        clusters = hdbscan.HDBSCAN(min_samples=min_samples, min_cluster_size=min_cluster_size)
        clusters.fit(embeddings_array)
    elif algorithm == KMEANS:
        print(f"Expected number of clusters: {expected_num_clusters}")
        clusters = KMeans(n_clusters=expected_num_clusters, init="k-means++", random_state=42)
        clusters.fit(embeddings_array)
    else:
        print(f"Error: Unsupported clustering algorithm: {algorithm}")
        return None

    return clusters


def sort_clusters(clusters, embeddings_array, metadata_list, update_records):
    cluster_representatives = {}
    cluster_sizes = {}
    cluster_info = []

    # Iterate through all unique cluster labels (excluding noise)
    for cluster_id in set(clusters.labels_):
        if cluster_id != -1:
            cluster_indices = np.where(clusters.labels_ == cluster_id)[0]
            cluster_size = len(cluster_indices)
            cluster_embeddings = embeddings_array[cluster_indices]
            cluster_mean = cluster_embeddings.mean(axis=0)

            # Find the closest point to the mean embedding
            closest_point_index = cluster_indices[np.argmin(np.linalg.norm(cluster_embeddings - cluster_mean, axis=1))]
            representative_metadata = metadata_list[closest_point_index]
            cluster_representatives[cluster_id] = representative_metadata["id"]
            cluster_sizes[cluster_id] = cluster_size

            # Store tuple of cluster information
            cluster_info.append((cluster_size, cluster_id, representative_metadata))

    if update_records:
        update_metadata_records(clusters, cluster_sizes, metadata_list, cluster_representatives)
    else:
        writeClusterInfo(clusters, cluster_sizes, metadata_list, cluster_representatives)

    # Sort by cluster size in descending order
    sorted_clusters = sorted(cluster_info, key=lambda x: x[0], reverse=True)

    return sorted_clusters


def writeClusterInfo(clusters, cluster_sizes, metadata_list, cluster_representatives):
    print("\nCluster info details")
    print("sourceId TAB clusterNumber TAB clusterSize TAB clusterCentroidId")

    for i, metadata in enumerate(metadata_list):
        cluster_id = clusters.labels_[i]
        if cluster_id != -1:
            print(f"{metadata['id']}\t{cluster_id}\t{cluster_sizes[cluster_id]}\t{cluster_representatives[cluster_id]}")
    print("End of cluster info details")

def update_metadata_records(clusters, cluster_sizes, metadata_list, cluster_representatives):
    print("Updating metadata records with cluster details\n")

    for i, metadata in enumerate(metadata_list):
        cluster_id = clusters.labels_[i]
        if cluster_id != -1:
            metadata["cluster_id"] = int(cluster_id)
            metadata["cluster_size"] = cluster_sizes[cluster_id]
            metadata["cluster_url"] = cluster_representatives[cluster_id]

            # Ensure 'file_path' is not written back
            file_path = metadata.pop("file_path", None)

            if file_path:
                print("Writing back " + file_path)
                with open(file_path, 'w') as f:                    
                    json.dump(metadata, f, indent=4)


def get_metadata(file_path):
    with open(file_path, 'r') as f:
        metadata = json.load(f)
        #  metadata = clean_metadata(metadata)

    return metadata


if __name__ == '__main__':
    main()
